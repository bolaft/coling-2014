%
% File coling2014.tex
%
% Contact: jwagner@computing.dcu.ie
%%
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%% e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}

\usepackage{coling2014}
\usepackage{times}
\usepackage{url}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc} 
\usepackage{latexsym}
\usepackage{upgreek}
\usepackage{bm}
\usepackage{todonotes}
\usepackage{csquotes}

\hypersetup{
    colorlinks=false,
    pdfborder={0 0 0},
}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\title{Act of speech boundary detection in email corpora}

\date{}

\author{}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}
\label{intro}

%
% The following footnote without marker is needed for the camera-ready
% version of the paper.
% Comment out the instructions (first text) and uncomment the 8 lines
% under final paper for your variant of English.
% 
\blfootnote{
    %
    % for review submission
    %
    \hspace{-0.65cm}  % space normally used by the marker
    % Place licence statement here for the camera-ready version, see
    % Section~\ref{licence} of the instructions for preparing a
    % manuscript.
    %
    % % final paper: en-uk version (to license, a licence)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licensed under a Creative Commons 
    % Attribution 4.0 International Licence.
    % Page numbers and proceedings footer are added by
    % the organisers.
    % Licence details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
    % 
    % % final paper: en-us version (to licence, a license)
    %
    % \hspace{-0.65cm}  % space normally used by the marker
    % This work is licenced under a Creative Commons 
    % Attribution 4.0 International License.
    % Page numbers and proceedings footer are added by
    % the organizers.
    % License details:
    % \url{http://creativecommons.org/licenses/by/4.0/}
}

\section{Related Work}

\section{Generating the annotations}

\section{Building the segmenter}

\subsection{Sequence labeling}

Our email speech act segmenter system is built around a linear-chain Conditional Random Field (CRF) classifier, as implemented in the sequence labelling toolkit Wapiti \cite{lavergne2010practical}. Each email is processed as a sequence and each sentence as an element of that sequence.

\subsection{Features for boundary detection}

The classifier uses features that capture graphic, orthographic, lexical and syntactic information about the sentence. Many of them are borrowed from related work in speech act classification \cite{qadir2011classifying} and email segmentation \cite{lampert2009segmenting}.

\subsubsection{Lexical and syntactic features}

\textbf{Ngrams:} we focus on the first and last three significant tokens in the sentence. Significance is determined by the number of occurrences of the token in the training corpus: terms with a frequency lower than \todo{fix me} 1/X are considered insignificant. 

If a sentence contains less than six significant tokens, the same token can be found in both triplets. For example, in the sentence \textit{``Have a good day !''}, the first three tokens would be \textit{``Have''}, \textit{``a''}, \textit{``good''} and the last three would be \textit{``good''}, \textit{``day''} and \textit{``!''}. If the sentence contains less than three significant tokens, missing values are replaced by a placeholder.

We define three individual features for the three unigrams, the two bigrams and the single trigram found in each of these sets. The features are the following: the unaltered form of the token (case-sensitive), the lemmatized form of the token (case-insensitive ; numbers present in the token are replaced by a special character) and the corresponding part-of-speech.

We use the Stanford Log-linear Part-Of-Speech Tagger for morpho-syntactic tagging \cite{toutanova2003feature}, and the WordNet lexical database to perform lemmatization \cite{miller1995wordnet}.

\textbf{Question words and interrogative ngrams:} one feature checks if a sentence begins with a ``wh*'' question word  (\textit{``who''}, \textit{``when''}, \textit{``where''}, \textit{``what''}, \textit{``which''}, \textit{``what''}, \textit{``how''}) or an ngram suggesting an incoming interrogation (e.g. \textit{``is it''} or \textit{``are there''}), and another one checks if the sentence merely contains such a word or ngram.

\textbf{Modals:} one feature indicates wether the sentence contains a modal (\textit{``may''}, \textit{``must''}, \textit{``shall''}, \textit{``will''}, \textit{``might''}, \textit{``should''}, \textit{``would''}, \textit{``could''}, and their negative forms).

\textbf{Plan phrases:} one feature looks for plan phrases (e.g. \textit{``i will''} or \textit{``we are going to''})

\textbf{Personal words:} three features check for first person, second person and third persons words, respectively (e.g. \textit{``we''}, \textit{``my''} and \textit{``me''} are recognized as first person words).

\textbf{First personal pronoun:} one feature records the first personal pronoun found in the sentence.

\textbf{First verbal form:} one feature records the tag of the first verbal form found in the sentence as classified by the Stanford Part-Of-Speech tagger ; that is an element of the Penn Treebank tag set \footnote{Alphabetical list of part-of-speech tags used in the Penn Treebank Project: \url{http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html}} (e.g. the feature \textit{``VBZ''} indicates a present tense verb in third person singular).

\subsubsection{Graphic features}

The following features' values are relative to other sentences. Four classes are defined for each feature: ``highest'' (top 25\%), ``high'' (top 50\%), ``low'' (bottom 50\%) and ``lowest'' (bottom 25\%).\newline

\textbf{Number of tokens:} the total number of tokens in the sentence, including insignificant ones.

\textbf{Number of characters:} the total number of characters in the sentence, including those in insignificant tokens.

\textbf{Average token length:} the average length of a token, including insignificant ones.

\textbf{Proportion of uppercase characters:} the proportion of uppercase characters in the sentence.

\textbf{Proportion of alphabetic characters:} the proportion of alphabetic characters in the sentence.

\textbf{Proportion of numeric characters:} the proportion of numeric characters in the sentence.

\subsubsection{Orthographic features}

\textbf{Number of greater-than signs:} the number of greater-than signs (``>''), also know as ``chevron'' symbols, in the sentence. Like previous features, this one is computed relatively to other sentences in the training set.

\textbf{Position:} the position of the sentence in the email.

\textbf{Question mark:} one feature checks if the sentence ends with a question mark, and another checks if it at least contains one.

\textbf{Colon:} one feature checks if the sentence ends with a colon, and another checks if it at least contains one.

\textbf{Semicolon:} one feature checks if the sentence ends with a semicolon, and another checks if it at least contains one.

\textbf{Early punctuation:} the last feature checks if the sentence contains any punctuation within the first three tokens of the sentence. This is meant to recognize greetings \cite{qadir2011classifying}.

\section{Experimental framework}

\subsection{Corpora}

\subsubsection{Ubuntu technical support email archive}

We use the \textit{ubuntu-users} email archive\footnote{\textit{ubuntu-users} mailing list archive: \url{https://lists.ubuntu.com/archives/ubuntu-users/}} as our primary corpus. It offers a number of advantages. It is free, and distributed under an unrestrictive license. It is up-to-date, with messages as recent as December 2013, and therefore is representative of modern emailing in both content and formatting conventions. Additionally, many alternatives archives are available\footnote{Ubuntu mailing lists archives: \url{https://lists.ubuntu.com/archives/}}, in a number of different languages, including some very resource-poor languages.

\subsubsection{BC3 email corpus}

\todo{fill me}

\subsection{Evaluation protocols}

We designed two evaluation protocols. The first one attempts to assess the segmenter's performance through 10-fold cross-validation on the Ubuntu corpus. The second uses a segmenter trained on the Ubuntu corpus to label the BC3 email corpus, and compares the resulting segmentation to a gold standard built \todo{finish me} from

\subsubsection{Metrics}

Traditional metrics of precision and recall are provided for all results. Precision is the percentage of boundaries identified by the classifier that are indeed true boundaries, recall is the percentage of true boundaries that are identified by the classifier. We also provide the $F_1$ score which represents the harmonic mean of precision and recall:

\[
    F_1 = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}
\]

However, automatic evaluation of speech segmentation through these metrics is problematic as predicted segment boundaries rarely align precisely. Moreover, while a segmenter that places boundaries near actual boundary positions is in almost all cases more suited to the task than one that misses by a much larger margin, precision and recall metrics would penalize both to the same extent. Therefore, in order to evaluate varying degrees of success or failure in a more subtle manner, we also provide an array of metrics relevant to the field of text segmentation : $\bm{P_{k}}$, WindowDiff and the Generalized Hamming Distance (GHD).

The $\bm{P_{k}}$ metric is a probabilistically motivated error metric for the assessment of segmentation algorithms \cite{beeferman1999statistical}.

\textbf{WindowDiff} compares the number of segment boundaries found within a fixed-sized window to the number of boundaries found in the same window of text for the reference segmentation \cite{pevzner2002critique}.

The \textbf{Generalized Hamming Distance (GHD)} is an extension of the Hamming distance\footnote{Wikipedia article on the Hamming distance: \url{http://en.wikipedia.org/wiki/Hamming_distance}} to give partial credit for near misses \cite{bookstein2002generalized}.

\subsubsection{Baselines}

Performance is compared to two simple heuristic baselines. The first one, the ``regular'' baseline, is computed by segmenting the test set into regular segments of the same length as the average training set segment length, rounded up. The second one, the ``irregular'' baseline, is computed by randomly inserting an appropriate number of segment boundaries in the test set (i.e. the boundary frequency stays similar to that of the training set). Results provided for the irregular baseline are calculated by computing the average scores of the first 1000 random segmentations.

\section{Experiments}

\subsection{Preprocessing}



\subsection{Email segmentation}

\subsection{Contributions of the segmenter to a speech act classification task}

\subsection{Results and discussion}

\section{Conclusion}

% include your own bib file like this:
\bibliographystyle{acl}
\bibliography{refs}

\end{document}
