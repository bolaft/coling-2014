
%------------------------------------------------------------------------------
\section{Building annotated corpora of segmented online discussions at no cost}
\label{}

%------------------------------------------------------------------------------
\subsection{Generate the annotations}
\label{}

\begin{enumerate}
\item 
\item FIXME provide an example of generation and resulting annotation and 
\end{enumerate}


We assume that a message can be split into %subsequent and
 consecutive discourse segments, each of them conveying its own dialogue act.
We assume the sentence as the elementary unit.
Consequently, each sentence in a segment can play one of the following roles: 
\textit{starting and ending} (\textit{SE}) a segment when there is only one sentence in the segment, 
\textit{starting} (\textit{S}) a segment if there are at least two sentences in the segment and the sentence is the first one, 
\textit{ending} (\textit{E}) a segment if there are at least two sentences in the segment and the sentence is the last one, and \textit{inside} (\textit{I}) a segment in any other cases.
%
As a matter of fact, this scheme is similar to the \textit{BIO} annotation scheme except it is at the sentence level and not at the token level.



When a message is replied to in e-mail, Internet forums, or Usenet, the original can often be included, or "quoted", in a variety of different posting styles.
%
The main options are {\em interleaved posting} (also called {\em inline replying}, in which the different parts of the reply follow the relevant parts of the original post), \textit{bottom-posting} (in which the reply follows the quote) or \textit{top-posting} (in which the reply precedes the quoted original message). 


FIXME


We define the following sentence labels: 
\begin{description}
\item [STARTEND] if the sentence of the source message is surrounded by insertions which are part of the reply message;
\item [START] else if the sentence of the source message is surrounded by insertions which are part of the reply message;
\item [END]
\item [INSIDE]
\end{description}
%Par exemple, on pourra étiqueter de \texttt{TERMINE} la phrase précédent un segment repris et de \texttt{DEBUTE} la première phrase du segment repris. Ces phrases ainsi annotées dans leur contexte constitueront notre corpus d'entraînement.


% -----------------------------------------------------------------------------
\subsection{Building the segmenter}


Our email speech act segmenter system is built around a linear-chain Conditional Random Field (CRF) classifier, as implemented in the sequence labelling toolkit Wapiti \cite{lavergne2010practical}. Each email is processed as a sequence and each sentence as an element of that sequence.

\subsubsection{Features for boundary detection}

The classifier uses features that capture graphic, orthographic, lexical and syntactic information about the sentence. Many of them are borrowed from related work in speech act classification \cite{qadir2011classifying} and email segmentation \cite{lampert2009segmenting}.

\subsubsection{Lexical and syntactic features}

\textbf{Ngrams:} we focus on the first and last three significant tokens in the sentence. Significance is determined by the number of occurrences of the token in the training corpus: terms with a frequency lower than \todo{fix me} 1/X are considered insignificant. 

If a sentence contains less than six significant tokens, the same token can be found in both triplets. For example, in the sentence \textit{``Have a good day !''}, the first three tokens would be \textit{``Have''}, \textit{``a''}, \textit{``good''} and the last three would be \textit{``good''}, \textit{``day''} and \textit{``!''}. If the sentence contains less than three significant tokens, missing values are replaced by a placeholder.

We define three individual features for the three unigrams, the two bigrams and the single trigram found in each of these sets. The features are the following: the unaltered form of the token (case-sensitive), the lemmatized form of the token (case-insensitive ; numbers present in the token are replaced by a special character) and the corresponding part-of-speech.

We use the Stanford Log-linear Part-Of-Speech Tagger for morpho-syntactic tagging \cite{toutanova2003feature}, and the WordNet lexical database to perform lemmatization \cite{miller1995wordnet}.

\textbf{Question words and interrogative ngrams:} one feature checks if a sentence begins with a ``wh*'' question word  (\textit{``who''}, \textit{``when''}, \textit{``where''}, \textit{``what''}, \textit{``which''}, \textit{``what''}, \textit{``how''}) or an ngram suggesting an incoming interrogation (e.g. \textit{``is it''} or \textit{``are there''}), and another one checks if the sentence merely contains such a word or ngram.

\textbf{Modals:} one feature indicates wether the sentence contains a modal (\textit{``may''}, \textit{``must''}, \textit{``shall''}, \textit{``will''}, \textit{``might''}, \textit{``should''}, \textit{``would''}, \textit{``could''}, and their negative forms).

\textbf{Plan phrases:} one feature looks for plan phrases (e.g. \textit{``i will''} or \textit{``we are going to''})

\textbf{Personal words:} three features check for first person, second person and third persons words, respectively (e.g. \textit{``we''}, \textit{``my''} and \textit{``me''} are recognized as first person words).

\textbf{First personal pronoun:} one feature records the first personal pronoun found in the sentence.

\textbf{First verbal form:} one feature records the tag of the first verbal form found in the sentence as classified by the Stanford Part-Of-Speech tagger ; that is an element of the Penn Treebank tag set \footnote{Alphabetical list of part-of-speech tags used in the Penn Treebank Project: \url{http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html}} (e.g. the feature \textit{``VBZ''} indicates a present tense verb in third person singular).

\subsubsection{Graphic features}

The following features' values are relative to other sentences. Four classes are defined for each feature: ``highest'' (top 25\%), ``high'' (top 50\%), ``low'' (bottom 50\%) and ``lowest'' (bottom 25\%).\newline

\textbf{Number of tokens:} the total number of tokens in the sentence, including insignificant ones.

\textbf{Number of characters:} the total number of characters in the sentence, including those in insignificant tokens.

\textbf{Average token length:} the average length of a token, including insignificant ones.

\textbf{Proportion of uppercase characters:} the proportion of uppercase characters in the sentence.

\textbf{Proportion of alphabetic characters:} the proportion of alphabetic characters in the sentence.

\textbf{Proportion of numeric characters:} the proportion of numeric characters in the sentence.

\subsubsection{Orthographic features}

\textbf{Number of greater-than signs:} the number of greater-than signs (``>''), also know as ``chevron'' symbols, in the sentence. Like previous features, this one is computed relatively to other sentences in the training set.

\textbf{Position:} the position of the sentence in the email.

\textbf{Question mark:} one feature checks if the sentence ends with a question mark, and another checks if it at least contains one.

\textbf{Colon:} one feature checks if the sentence ends with a colon, and another checks if it at least contains one.

\textbf{Semicolon:} one feature checks if the sentence ends with a semicolon, and another checks if it at least contains one.

\textbf{Early punctuation:} the last feature checks if the sentence contains any punctuation within the first three tokens of the sentence. This is meant to recognize greetings \cite{qadir2011classifying}.

