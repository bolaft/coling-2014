
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

\section{Experiments}
\label{sec:experiments}

% FIXME Table \ref{fig:results} présente / résume / fusionne tous les résultats ; phrase qui annonce les différentes étapes

Table\ref{fig:results} shows the summary of all obtained results. On the left side are shown results for segmentation metrics, on the right side results for information retrieval metrics. First, we examine baseline scores, and display them in the top section of Table\ref{fig:results}. Second, in the middle section, we show results for segmenters based on individual feature sets. Finally, in the lower section, we show results based on feature sets combinations.

\begin{table}[h]
	\begin{tabular}{p{2.5cm}|C{1.5cm}|C{1.5cm}|C{1.5cm}||C{1.5cm}|C{1.5cm}|C{1.5cm}|}
		\cline{2-7}
		& \multicolumn{3}{c||}{Segmentation metrics} & \multicolumn{3}{c|}{IR metrics} \\ \cline{2-7}
		& \textit{WD} & $P_{k}$ & \textit{GHD} & \textit{Precision} & \textit{Recall} & \textit{$F_1$} \\ \hline
		\multicolumn{1}{|l|}{regular baseline} 					& .59 & .25 & .60 & .31 & .49 & .38  \\ \hline
		\multicolumn{1}{|l|}{TextTiling baseline} 				& .41 & .07 & .38 & .75 & .44 & .56 \\ \hline\hline
		\multicolumn{1}{|l|}{$n$-grams} 						& .38 & \textbf{.05} & .39 & \textbf{1} & .39 & .56 \\ \hline
		\multicolumn{1}{|l|}{information structure} 			& .43 & .11 & .38 & .60 & .68 & \textbf{.64} \\ \hline
		\multicolumn{1}{|l|}{thematic (TextTiling)}				& .39 & .05 & .38 & .94 & .40 & .56 \\ \hline
		\multicolumn{1}{|l|}{miscellaneous} 					& .41 & .09 & .38 & .69 & .49 & .57 \\ \hline\hline
		\multicolumn{1}{|l|}{all features} 						& .38 & \textbf{.05} & .39 & \textbf{1} & .39 & .56\\ \hline
		\multicolumn{1}{|l|}{$n$-grams $\cup$ non-$n$-grams} 	& .45 & .12 & .40 & .58 & \textbf{.69} & .63 \\ \hline
		\multicolumn{1}{|l|}{$n$-grams $\cup$ "cherry-pick"} 	& \textbf{.36} & .06 & \textbf{.34} & .80 & .53 & \textbf{.64} \\ \hline
	\end{tabular}
	\caption{Comparative results for segmenters and baselines. All displayed results show \textit{WindowDiff} (\textit{WD}), $P_{k}$ and \textit{GHD} as error rates, therefore a lower score is desirable for these metrics. This contrasts with the three information retrieval scores, for which a low value denotes poor performance.}
	\label{fig:results}
\end{table}

\subsection{Baseline segmenters}

The first section of Table \ref{fig:results} shows the results obtained by both of our baselines. Unsurprisingly, TextTiling performs much better than the basic regular segmentation algorithm across all metrics.

\subsection{Segmenters based on individual feature sets}

The second section of Table \ref{fig:results} shows the results for four different classifiers, each trained with a distinct subset of the feature set. While all classifiers easily beat the regular baseline, and the TextTiling baseline when it comes to information retrieval metrics, only the thematic and the $n$-grams segmenters manage to surpass TextTiling when performance is measured by segmentation metrics. In terms of IR scores, the $n$-grams classifier in particular stands out as it manage to achieve an outstanding 100\% precision, although this result is mitigated by a meager 39\% recall. It is also interesting to see that the thematic classifier, based only on contextual information about TextTiling output, performs better than the TextTiling baseline.

\subsection{Segmenters based on feature sets combinations}

The last section of Table \ref{fig:results} shows the results of three different segmenters. The first one is simply a classifier that takes all available features into account. Its results are exactly identical to that of the $n$-grams classifiers, most certainly due to the fact that other features are filtered out due to the sheer number of lexical features. The second one segments according to the union of the boundaries detected by a classifier trained on lexical features and those identified by a classifier trained on all other features. Doing this allows the segmenter to obtain the best possible recall, but at the expense of precision. The last one, "$n$-grams $\cup$ cherry-pick"  considers all boundaries detected by a classifier trained on lexical features as correct, and selects only the boundaries identified by a classifier trained on all other features with a confidence score of 99\% or more. This system outperforms all others both in terms of segmentation scores and $F_1$, however it is still relatively conservative and the segmentation ratio (the number of true boundaries divided by the number of guessed boundaries) remain significantly lower than expected, at~0.67.
